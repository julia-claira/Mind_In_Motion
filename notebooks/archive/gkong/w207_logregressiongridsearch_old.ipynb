{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melia to insert code here\n",
    "def run_logregression_experiment(df_depression=df_depression,\n",
    "                                  n_features=20,\n",
    "                                  c=1.0,\n",
    "                                  penalty='l2',\n",
    "                                  solver='saga'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Runs logistic regression with cross-validation. Returns a dataframe with average training accuracies, average validation accuracies, and average variance across folds\n",
    "\n",
    "    Params:\n",
    "        df_depression: dataframe with depression data\n",
    "        n_features: number of features to use\n",
    "        c: inverse of regularization strength\n",
    "        penalty: type of regularization\n",
    "        solver: algorithm to use in the optimization problem\n",
    "\n",
    "    Returns:\n",
    "        df_results: dataframe with results of experiment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Print number of features used\n",
    "    print(f\"\\nRunning experiment with {n_features} features, c={c}, penalty={penalty}, solver={solver}\")\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "    # Calculate column index for last feature\n",
    "    last_feature_index = n_features + 1\n",
    "    \n",
    "    # Split the data into X and y\n",
    "    X = np.array(df_depression.iloc[:, 1:last_feature_index])\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    y = np.array(df_depression.iloc[:, 0])\n",
    "\n",
    "    # Define the hyperparameter grid to search\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': [100, 500, 1000]\n",
    "    }\n",
    "\n",
    "    # Create the logistic regression model\n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "\n",
    "    # define k-fold cross-validation experimentation\n",
    "    k_folds = 5\n",
    "    cv = KFold(n_splits=k_folds, shuffle=True, random_state=1)\n",
    "\n",
    "    # Perform the grid search\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_std, y)\n",
    "\n",
    "    # Get the best hyperparameters and the corresponding mean test score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_mean_test_score = grid_search.best_score_\n",
    "\n",
    "    # Lists to store training and validation scores for each fold\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    # Perform cross-validation and get training and validation scores for each fold\n",
    "    for train_index, validation_index in cv.split(X_std, y):\n",
    "        X_train, X_val = X_std[train_index], X_std[validation_index]\n",
    "        y_train, y_val = y[train_index], y[validation_index]\n",
    "\n",
    "        # Train the model on the training set within each fold\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Calculate training and validation accuracies\n",
    "        train_accuracy = model.score(X_train, y_train)\n",
    "        validation_accuracy = model.score(X_val, y_val)\n",
    "\n",
    "        # Append the accuracies to the lists\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "        # Print the accuracy for the current fold\n",
    "        print(f\"Fold {len(train_accuracies)} - Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    # Calculate average training and validation accuracies\n",
    "    average_training_accuracy = np.mean(train_accuracies)\n",
    "    average_validation_accuracy = np.mean(validation_accuracies)\n",
    "    average_variance = average_training_accuracy - average_validation_accuracy \n",
    "\n",
    "    # Print the overall statistics\n",
    "    print(\"Average Training Accuracy:\", average_training_accuracy)\n",
    "    print(\"Average Validation Accuracy:\", average_validation_accuracy)\n",
    "\n",
    "    # Create a dataframe containing results of the experiment\n",
    "    df_results = pd.DataFrame({\n",
    "        'n_features': [n_features],\n",
    "        'c': [c],\n",
    "        'penalty': [penalty],\n",
    "        'solver': [solver],\n",
    "        'average_training_accuracy': [average_training_accuracy],\n",
    "        'average_validation_accuracy': [average_validation_accuracy],\n",
    "        'average_variance': [average_variance]\n",
    "    })\n",
    "\n",
    "    return df_results\n",
    "\n",
    "# Call the function to perform the grid search\n",
    "df_results = run_logregression_experiment(df_depression, n_features=20)\n",
    "\n",
    "# Display the complete DataFrame of results\n",
    "print(df_results)\n",
    "\n",
    "# Find the row that has the lowest variance and highest validation accuracy\n",
    "best_row = df_results.loc[df_results['average_variance'].idxmin()]\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_adaboost(X_std, y):\n",
    "    # Perform algorithm on no split data with cross validation\n",
    "    X, y = X_std, y\n",
    "    base = DecisionTreeClassifier(max_depth=1,criterion='entropy')\n",
    "    model = AdaBoostClassifier(estimator=base)\n",
    "\n",
    "    # Define the grid of values to search\n",
    "    grid = dict()\n",
    "    grid['n_estimators'] = [700,800,710,730,750,780,770, 778,783]\n",
    "    grid['learning_rate'] = [1.7,1.8,1.9,1.75]\n",
    "\n",
    "    # Define the evaluation procedure\n",
    "    k_folds = 5\n",
    "    cv = KFold(n_splits=k_folds, shuffle=True, random_state=1)\n",
    "\n",
    "    # Define the grid search procedure\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',return_train_score=True)\n",
    "\n",
    "    # Execute the grid search\n",
    "    grid_result = grid_search.fit(X, y)\n",
    "\n",
    "    # Summarize the best score and configuration\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "    # # Summarize all scores that were evaluated\n",
    "    # means = grid_result.cv_results_['mean_test_score']\n",
    "    # stds = grid_result.cv_results_['std_test_score']\n",
    "    # params = grid_result.cv_results_['params']\n",
    "    # for mean, stdev, param in zip(means, stds, params):\n",
    "    #     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    # Convert cv_results_ to a pandas DataFrame\n",
    "    cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    # Remove rows from cv_results_ that have NaN mean_test_score\n",
    "    cv_results_df = cv_results_df[cv_results_df['mean_test_score'].notna()]\n",
    "\n",
    "    # Calculate mean variance (difference between test and train) for each hyperparameter combination\n",
    "    cv_results_df['mean_variance'] = cv_results_df['mean_train_score'] - cv_results_df['mean_test_score']\n",
    "\n",
    "    # Keep only the columns of interest\n",
    "    cv_results_df = cv_results_df[['param_n_estimators', 'param_learning_rate', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score', 'mean_variance']]\n",
    "\n",
    "    # Sort cv_results_ by mean_test_score and mean_variance(inverted) and reindex\n",
    "    cv_results_df = cv_results_df.sort_values(by=['mean_test_score', 'mean_variance'], ascending=[False, True])\n",
    "    cv_results_df = cv_results_df.reset_index(drop=True)\n",
    "\n",
    "    return cv_results_df, grid_result.best_params_\n",
    "\n",
    "df_gridsearch_adaboost, adaboost_bestparams = grid_search_adaboost(X_std, y)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
