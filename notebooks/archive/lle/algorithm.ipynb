{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from mlxtend.plotting import heatmap\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "# datetime for manipulating date and time\n",
    "import datetime\n",
    "\n",
    "# statsmodels for autoregressive models\n",
    "import statsmodels.tsa.ar_model as AutoReg\n",
    "\n",
    "# scipy\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Tree Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>number</th>\n",
       "      <th>madrs2_grouped</th>\n",
       "      <th>madrs2_grouped_ord</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Mild</th>\n",
       "      <th>Moderate</th>\n",
       "      <th>activity_intraday_variability</th>\n",
       "      <th>activity_mean</th>\n",
       "      <th>activity_cv_minutely</th>\n",
       "      <th>...</th>\n",
       "      <th>act_bout_num</th>\n",
       "      <th>act_bout_dur_mean</th>\n",
       "      <th>act_bout_dur_std</th>\n",
       "      <th>act_bout_dur_cv</th>\n",
       "      <th>act_bout_dur_skew</th>\n",
       "      <th>act_bout_dur_entropy</th>\n",
       "      <th>inact_bout_dur_mean</th>\n",
       "      <th>inact_bout_dur_cv</th>\n",
       "      <th>inact_bout_dur_skew</th>\n",
       "      <th>inact_bout_dur_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>control_3</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>450.847169</td>\n",
       "      <td>251.233880</td>\n",
       "      <td>1.409631</td>\n",
       "      <td>...</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>16.975000</td>\n",
       "      <td>17.689084</td>\n",
       "      <td>1.042067</td>\n",
       "      <td>2.667753</td>\n",
       "      <td>5.687968</td>\n",
       "      <td>31.866667</td>\n",
       "      <td>3.549919</td>\n",
       "      <td>7.853831</td>\n",
       "      <td>4.095484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>control_19</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>497.025070</td>\n",
       "      <td>226.506920</td>\n",
       "      <td>1.504780</td>\n",
       "      <td>...</td>\n",
       "      <td>36.384615</td>\n",
       "      <td>13.955603</td>\n",
       "      <td>15.838704</td>\n",
       "      <td>1.134935</td>\n",
       "      <td>3.636279</td>\n",
       "      <td>5.738435</td>\n",
       "      <td>28.545679</td>\n",
       "      <td>2.372276</td>\n",
       "      <td>4.508271</td>\n",
       "      <td>4.832628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>control_25</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>578.504177</td>\n",
       "      <td>316.378243</td>\n",
       "      <td>1.353900</td>\n",
       "      <td>...</td>\n",
       "      <td>30.461538</td>\n",
       "      <td>25.111111</td>\n",
       "      <td>75.382117</td>\n",
       "      <td>3.001943</td>\n",
       "      <td>16.823950</td>\n",
       "      <td>5.101094</td>\n",
       "      <td>29.782738</td>\n",
       "      <td>2.313598</td>\n",
       "      <td>4.512948</td>\n",
       "      <td>4.707541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>control_12</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>341.117566</td>\n",
       "      <td>147.202370</td>\n",
       "      <td>1.597837</td>\n",
       "      <td>...</td>\n",
       "      <td>28.285714</td>\n",
       "      <td>12.378788</td>\n",
       "      <td>14.921837</td>\n",
       "      <td>1.205436</td>\n",
       "      <td>4.124580</td>\n",
       "      <td>5.541758</td>\n",
       "      <td>36.585082</td>\n",
       "      <td>2.519433</td>\n",
       "      <td>9.393374</td>\n",
       "      <td>4.984534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>condition_2</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>566.183307</td>\n",
       "      <td>211.694251</td>\n",
       "      <td>1.773271</td>\n",
       "      <td>...</td>\n",
       "      <td>32.055556</td>\n",
       "      <td>15.303293</td>\n",
       "      <td>61.551835</td>\n",
       "      <td>4.022130</td>\n",
       "      <td>21.570781</td>\n",
       "      <td>5.341824</td>\n",
       "      <td>34.537698</td>\n",
       "      <td>2.350872</td>\n",
       "      <td>4.200135</td>\n",
       "      <td>4.979392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       number madrs2_grouped  madrs2_grouped_ord  Normal  Mild  \\\n",
       "0          45    control_3         Normal                   0       1     0   \n",
       "1          33   control_19         Normal                   0       1     0   \n",
       "2          40   control_25         Normal                   0       1     0   \n",
       "3          26   control_12         Normal                   0       1     0   \n",
       "4          11  condition_2           Mild                   1       0     1   \n",
       "\n",
       "   Moderate  activity_intraday_variability  activity_mean  \\\n",
       "0         0                     450.847169     251.233880   \n",
       "1         0                     497.025070     226.506920   \n",
       "2         0                     578.504177     316.378243   \n",
       "3         0                     341.117566     147.202370   \n",
       "4         0                     566.183307     211.694251   \n",
       "\n",
       "   activity_cv_minutely  ...  act_bout_num  act_bout_dur_mean  \\\n",
       "0              1.409631  ...     36.666667          16.975000   \n",
       "1              1.504780  ...     36.384615          13.955603   \n",
       "2              1.353900  ...     30.461538          25.111111   \n",
       "3              1.597837  ...     28.285714          12.378788   \n",
       "4              1.773271  ...     32.055556          15.303293   \n",
       "\n",
       "   act_bout_dur_std  act_bout_dur_cv  act_bout_dur_skew  act_bout_dur_entropy  \\\n",
       "0         17.689084         1.042067           2.667753              5.687968   \n",
       "1         15.838704         1.134935           3.636279              5.738435   \n",
       "2         75.382117         3.001943          16.823950              5.101094   \n",
       "3         14.921837         1.205436           4.124580              5.541758   \n",
       "4         61.551835         4.022130          21.570781              5.341824   \n",
       "\n",
       "   inact_bout_dur_mean  inact_bout_dur_cv  inact_bout_dur_skew  \\\n",
       "0            31.866667           3.549919             7.853831   \n",
       "1            28.545679           2.372276             4.508271   \n",
       "2            29.782738           2.313598             4.512948   \n",
       "3            36.585082           2.519433             9.393374   \n",
       "4            34.537698           2.350872             4.200135   \n",
       "\n",
       "   inact_bout_dur_entropy  \n",
       "0                4.095484  \n",
       "1                4.832628  \n",
       "2                4.707541  \n",
       "3                4.984534  \n",
       "4                4.979392  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = pd.read_csv(\"../../notebooks/lle/features_algorithm.csv\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>number</th>\n",
       "      <th>madrs2_grouped</th>\n",
       "      <th>madrs2_grouped_ord</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Mild</th>\n",
       "      <th>Moderate</th>\n",
       "      <th>activity_intraday_variability</th>\n",
       "      <th>activity_mean</th>\n",
       "      <th>activity_cv_minutely</th>\n",
       "      <th>...</th>\n",
       "      <th>act_bout_num</th>\n",
       "      <th>act_bout_dur_mean</th>\n",
       "      <th>act_bout_dur_std</th>\n",
       "      <th>act_bout_dur_cv</th>\n",
       "      <th>act_bout_dur_skew</th>\n",
       "      <th>act_bout_dur_entropy</th>\n",
       "      <th>inact_bout_dur_mean</th>\n",
       "      <th>inact_bout_dur_cv</th>\n",
       "      <th>inact_bout_dur_skew</th>\n",
       "      <th>inact_bout_dur_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>36</td>\n",
       "      <td>control_21</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>501.256833</td>\n",
       "      <td>264.990354</td>\n",
       "      <td>1.390468</td>\n",
       "      <td>...</td>\n",
       "      <td>38.750000</td>\n",
       "      <td>15.570968</td>\n",
       "      <td>20.479364</td>\n",
       "      <td>1.315227</td>\n",
       "      <td>5.337884</td>\n",
       "      <td>5.238865</td>\n",
       "      <td>21.813880</td>\n",
       "      <td>2.368584</td>\n",
       "      <td>5.583540</td>\n",
       "      <td>4.729848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>13</td>\n",
       "      <td>condition_21</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>581.136493</td>\n",
       "      <td>79.936852</td>\n",
       "      <td>2.697304</td>\n",
       "      <td>...</td>\n",
       "      <td>17.923077</td>\n",
       "      <td>9.236052</td>\n",
       "      <td>9.194736</td>\n",
       "      <td>0.995527</td>\n",
       "      <td>2.322436</td>\n",
       "      <td>5.096657</td>\n",
       "      <td>64.707143</td>\n",
       "      <td>2.424348</td>\n",
       "      <td>4.328888</td>\n",
       "      <td>4.389412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>12</td>\n",
       "      <td>condition_20</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>375.133788</td>\n",
       "      <td>69.371695</td>\n",
       "      <td>2.351840</td>\n",
       "      <td>...</td>\n",
       "      <td>16.153846</td>\n",
       "      <td>7.628571</td>\n",
       "      <td>5.674179</td>\n",
       "      <td>0.743806</td>\n",
       "      <td>2.332610</td>\n",
       "      <td>5.132767</td>\n",
       "      <td>46.642857</td>\n",
       "      <td>1.954293</td>\n",
       "      <td>4.103701</td>\n",
       "      <td>5.017741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>condition_14</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>415.016644</td>\n",
       "      <td>75.205611</td>\n",
       "      <td>2.487839</td>\n",
       "      <td>...</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>7.894737</td>\n",
       "      <td>7.177810</td>\n",
       "      <td>0.909189</td>\n",
       "      <td>4.969473</td>\n",
       "      <td>5.324928</td>\n",
       "      <td>47.501312</td>\n",
       "      <td>1.585688</td>\n",
       "      <td>5.739879</td>\n",
       "      <td>5.259853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>condition_19</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>514.936507</td>\n",
       "      <td>165.284240</td>\n",
       "      <td>1.766114</td>\n",
       "      <td>...</td>\n",
       "      <td>33.461538</td>\n",
       "      <td>10.245977</td>\n",
       "      <td>11.835146</td>\n",
       "      <td>1.155102</td>\n",
       "      <td>3.084005</td>\n",
       "      <td>5.645285</td>\n",
       "      <td>29.305439</td>\n",
       "      <td>2.279229</td>\n",
       "      <td>4.874779</td>\n",
       "      <td>5.136790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>control_25</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>578.504177</td>\n",
       "      <td>316.378243</td>\n",
       "      <td>1.353900</td>\n",
       "      <td>...</td>\n",
       "      <td>30.461538</td>\n",
       "      <td>25.111111</td>\n",
       "      <td>75.382117</td>\n",
       "      <td>3.001943</td>\n",
       "      <td>16.823950</td>\n",
       "      <td>5.101094</td>\n",
       "      <td>29.782738</td>\n",
       "      <td>2.313598</td>\n",
       "      <td>4.512948</td>\n",
       "      <td>4.707541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8</td>\n",
       "      <td>condition_17</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>280.946142</td>\n",
       "      <td>86.749194</td>\n",
       "      <td>1.795802</td>\n",
       "      <td>...</td>\n",
       "      <td>22.692308</td>\n",
       "      <td>11.037288</td>\n",
       "      <td>72.976806</td>\n",
       "      <td>6.611842</td>\n",
       "      <td>17.084481</td>\n",
       "      <td>4.050248</td>\n",
       "      <td>39.730310</td>\n",
       "      <td>2.243364</td>\n",
       "      <td>4.879217</td>\n",
       "      <td>5.030853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6</td>\n",
       "      <td>condition_15</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>446.762140</td>\n",
       "      <td>117.579443</td>\n",
       "      <td>2.027575</td>\n",
       "      <td>...</td>\n",
       "      <td>23.769231</td>\n",
       "      <td>10.097087</td>\n",
       "      <td>10.989234</td>\n",
       "      <td>1.088357</td>\n",
       "      <td>3.596046</td>\n",
       "      <td>5.359727</td>\n",
       "      <td>38.069307</td>\n",
       "      <td>2.371731</td>\n",
       "      <td>4.677363</td>\n",
       "      <td>4.865471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>21</td>\n",
       "      <td>condition_8</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>702.131570</td>\n",
       "      <td>218.154069</td>\n",
       "      <td>1.942973</td>\n",
       "      <td>...</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>14.133758</td>\n",
       "      <td>18.433907</td>\n",
       "      <td>1.304247</td>\n",
       "      <td>2.852017</td>\n",
       "      <td>4.502515</td>\n",
       "      <td>39.200000</td>\n",
       "      <td>1.957940</td>\n",
       "      <td>3.703643</td>\n",
       "      <td>4.016461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20</td>\n",
       "      <td>condition_7</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>651.798586</td>\n",
       "      <td>322.489322</td>\n",
       "      <td>1.511731</td>\n",
       "      <td>...</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>21.104895</td>\n",
       "      <td>50.033913</td>\n",
       "      <td>2.370726</td>\n",
       "      <td>5.344657</td>\n",
       "      <td>4.613154</td>\n",
       "      <td>36.039735</td>\n",
       "      <td>3.064516</td>\n",
       "      <td>8.197120</td>\n",
       "      <td>4.311731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>condition_2</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>566.183307</td>\n",
       "      <td>211.694251</td>\n",
       "      <td>1.773271</td>\n",
       "      <td>...</td>\n",
       "      <td>32.055556</td>\n",
       "      <td>15.303293</td>\n",
       "      <td>61.551835</td>\n",
       "      <td>4.022130</td>\n",
       "      <td>21.570781</td>\n",
       "      <td>5.341824</td>\n",
       "      <td>34.537698</td>\n",
       "      <td>2.350872</td>\n",
       "      <td>4.200135</td>\n",
       "      <td>4.979392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>condition_13</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>487.534039</td>\n",
       "      <td>262.156813</td>\n",
       "      <td>1.385956</td>\n",
       "      <td>...</td>\n",
       "      <td>41.928571</td>\n",
       "      <td>13.521295</td>\n",
       "      <td>15.757683</td>\n",
       "      <td>1.165398</td>\n",
       "      <td>3.217821</td>\n",
       "      <td>5.918785</td>\n",
       "      <td>25.356415</td>\n",
       "      <td>3.116847</td>\n",
       "      <td>10.843546</td>\n",
       "      <td>4.908043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16</td>\n",
       "      <td>condition_3</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>538.448261</td>\n",
       "      <td>266.215983</td>\n",
       "      <td>1.499762</td>\n",
       "      <td>...</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>20.410000</td>\n",
       "      <td>66.217133</td>\n",
       "      <td>3.244348</td>\n",
       "      <td>17.668665</td>\n",
       "      <td>5.103950</td>\n",
       "      <td>32.049587</td>\n",
       "      <td>2.002618</td>\n",
       "      <td>4.440771</td>\n",
       "      <td>4.918294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18</td>\n",
       "      <td>condition_5</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>479.816472</td>\n",
       "      <td>173.693090</td>\n",
       "      <td>1.731070</td>\n",
       "      <td>...</td>\n",
       "      <td>27.153846</td>\n",
       "      <td>13.059490</td>\n",
       "      <td>17.089150</td>\n",
       "      <td>1.308562</td>\n",
       "      <td>3.092334</td>\n",
       "      <td>5.332214</td>\n",
       "      <td>32.864286</td>\n",
       "      <td>1.959681</td>\n",
       "      <td>4.490926</td>\n",
       "      <td>5.170926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>control_32</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>564.941965</td>\n",
       "      <td>160.257003</td>\n",
       "      <td>1.853685</td>\n",
       "      <td>...</td>\n",
       "      <td>24.785714</td>\n",
       "      <td>12.720461</td>\n",
       "      <td>12.421300</td>\n",
       "      <td>0.976482</td>\n",
       "      <td>2.539530</td>\n",
       "      <td>5.489731</td>\n",
       "      <td>42.167116</td>\n",
       "      <td>2.566225</td>\n",
       "      <td>4.999590</td>\n",
       "      <td>4.698126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>39</td>\n",
       "      <td>control_24</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>958.001261</td>\n",
       "      <td>377.208889</td>\n",
       "      <td>1.700495</td>\n",
       "      <td>...</td>\n",
       "      <td>32.692308</td>\n",
       "      <td>20.138824</td>\n",
       "      <td>27.367382</td>\n",
       "      <td>1.358936</td>\n",
       "      <td>4.123580</td>\n",
       "      <td>5.508435</td>\n",
       "      <td>32.023810</td>\n",
       "      <td>3.579543</td>\n",
       "      <td>7.418587</td>\n",
       "      <td>4.111767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>54</td>\n",
       "      <td>control_9</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>456.019285</td>\n",
       "      <td>138.380029</td>\n",
       "      <td>1.899449</td>\n",
       "      <td>...</td>\n",
       "      <td>26.384615</td>\n",
       "      <td>11.075802</td>\n",
       "      <td>10.882148</td>\n",
       "      <td>0.982516</td>\n",
       "      <td>3.207440</td>\n",
       "      <td>5.498305</td>\n",
       "      <td>40.630380</td>\n",
       "      <td>2.853196</td>\n",
       "      <td>8.598373</td>\n",
       "      <td>4.759101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43</td>\n",
       "      <td>control_28</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>561.622641</td>\n",
       "      <td>293.018097</td>\n",
       "      <td>1.394358</td>\n",
       "      <td>...</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>53.555784</td>\n",
       "      <td>3.758301</td>\n",
       "      <td>24.874405</td>\n",
       "      <td>5.783849</td>\n",
       "      <td>16.941345</td>\n",
       "      <td>1.810419</td>\n",
       "      <td>4.703947</td>\n",
       "      <td>5.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31</td>\n",
       "      <td>control_17</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>463.476496</td>\n",
       "      <td>240.285020</td>\n",
       "      <td>1.406504</td>\n",
       "      <td>...</td>\n",
       "      <td>40.111111</td>\n",
       "      <td>14.160665</td>\n",
       "      <td>24.128745</td>\n",
       "      <td>1.703927</td>\n",
       "      <td>10.282041</td>\n",
       "      <td>5.340840</td>\n",
       "      <td>24.403125</td>\n",
       "      <td>2.448441</td>\n",
       "      <td>5.119227</td>\n",
       "      <td>4.657088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>control_15</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>602.243602</td>\n",
       "      <td>295.178077</td>\n",
       "      <td>1.415641</td>\n",
       "      <td>...</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>20.903226</td>\n",
       "      <td>28.050994</td>\n",
       "      <td>1.341946</td>\n",
       "      <td>4.082569</td>\n",
       "      <td>5.291063</td>\n",
       "      <td>39.979757</td>\n",
       "      <td>2.904364</td>\n",
       "      <td>8.091174</td>\n",
       "      <td>4.159221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>17</td>\n",
       "      <td>condition_4</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>578.098625</td>\n",
       "      <td>291.488963</td>\n",
       "      <td>1.559481</td>\n",
       "      <td>...</td>\n",
       "      <td>31.846154</td>\n",
       "      <td>16.789855</td>\n",
       "      <td>29.264376</td>\n",
       "      <td>1.742980</td>\n",
       "      <td>5.130374</td>\n",
       "      <td>5.306521</td>\n",
       "      <td>25.475845</td>\n",
       "      <td>2.560030</td>\n",
       "      <td>5.353134</td>\n",
       "      <td>4.880975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>control_6</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>717.492149</td>\n",
       "      <td>346.517833</td>\n",
       "      <td>1.443704</td>\n",
       "      <td>...</td>\n",
       "      <td>38.307692</td>\n",
       "      <td>16.128514</td>\n",
       "      <td>20.642681</td>\n",
       "      <td>1.279887</td>\n",
       "      <td>3.562979</td>\n",
       "      <td>5.698802</td>\n",
       "      <td>24.259542</td>\n",
       "      <td>2.804492</td>\n",
       "      <td>5.472237</td>\n",
       "      <td>4.658652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14</td>\n",
       "      <td>condition_22</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>396.144454</td>\n",
       "      <td>162.486041</td>\n",
       "      <td>1.563656</td>\n",
       "      <td>...</td>\n",
       "      <td>34.642857</td>\n",
       "      <td>14.377320</td>\n",
       "      <td>59.781730</td>\n",
       "      <td>4.158058</td>\n",
       "      <td>20.792508</td>\n",
       "      <td>5.218089</td>\n",
       "      <td>28.060120</td>\n",
       "      <td>1.901712</td>\n",
       "      <td>3.418467</td>\n",
       "      <td>5.275094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>429.453604</td>\n",
       "      <td>155.345969</td>\n",
       "      <td>1.761592</td>\n",
       "      <td>...</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>13.479339</td>\n",
       "      <td>66.599417</td>\n",
       "      <td>4.940852</td>\n",
       "      <td>18.155197</td>\n",
       "      <td>4.664714</td>\n",
       "      <td>30.368286</td>\n",
       "      <td>2.586638</td>\n",
       "      <td>5.029551</td>\n",
       "      <td>4.747316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>25</td>\n",
       "      <td>control_11</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>551.650117</td>\n",
       "      <td>193.321593</td>\n",
       "      <td>1.730879</td>\n",
       "      <td>...</td>\n",
       "      <td>34.153846</td>\n",
       "      <td>14.506757</td>\n",
       "      <td>68.969666</td>\n",
       "      <td>4.754313</td>\n",
       "      <td>19.954122</td>\n",
       "      <td>4.928668</td>\n",
       "      <td>27.261411</td>\n",
       "      <td>2.264217</td>\n",
       "      <td>5.110121</td>\n",
       "      <td>5.149185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>49</td>\n",
       "      <td>control_4</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>714.953073</td>\n",
       "      <td>207.433702</td>\n",
       "      <td>1.880670</td>\n",
       "      <td>...</td>\n",
       "      <td>30.538462</td>\n",
       "      <td>11.528967</td>\n",
       "      <td>11.929392</td>\n",
       "      <td>1.034732</td>\n",
       "      <td>3.208551</td>\n",
       "      <td>5.617613</td>\n",
       "      <td>26.201161</td>\n",
       "      <td>2.118761</td>\n",
       "      <td>4.954406</td>\n",
       "      <td>5.286521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>condition_11</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>542.382958</td>\n",
       "      <td>132.735775</td>\n",
       "      <td>2.113054</td>\n",
       "      <td>...</td>\n",
       "      <td>23.571429</td>\n",
       "      <td>10.981818</td>\n",
       "      <td>9.170107</td>\n",
       "      <td>0.835026</td>\n",
       "      <td>1.772614</td>\n",
       "      <td>5.514034</td>\n",
       "      <td>45.690608</td>\n",
       "      <td>2.469748</td>\n",
       "      <td>4.516316</td>\n",
       "      <td>4.637721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>53</td>\n",
       "      <td>control_8</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>595.110233</td>\n",
       "      <td>407.068605</td>\n",
       "      <td>1.222877</td>\n",
       "      <td>...</td>\n",
       "      <td>32.923077</td>\n",
       "      <td>28.392523</td>\n",
       "      <td>74.975001</td>\n",
       "      <td>2.640660</td>\n",
       "      <td>15.835934</td>\n",
       "      <td>5.242367</td>\n",
       "      <td>31.210317</td>\n",
       "      <td>2.289926</td>\n",
       "      <td>3.721090</td>\n",
       "      <td>4.328408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>37</td>\n",
       "      <td>control_22</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>560.200512</td>\n",
       "      <td>257.622303</td>\n",
       "      <td>1.500055</td>\n",
       "      <td>...</td>\n",
       "      <td>37.615385</td>\n",
       "      <td>14.312883</td>\n",
       "      <td>17.048696</td>\n",
       "      <td>1.191143</td>\n",
       "      <td>4.580120</td>\n",
       "      <td>5.744807</td>\n",
       "      <td>25.157407</td>\n",
       "      <td>2.585875</td>\n",
       "      <td>5.206024</td>\n",
       "      <td>4.884087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>47</td>\n",
       "      <td>control_31</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>795.174523</td>\n",
       "      <td>367.441441</td>\n",
       "      <td>1.483490</td>\n",
       "      <td>...</td>\n",
       "      <td>31.230769</td>\n",
       "      <td>20.581281</td>\n",
       "      <td>26.426082</td>\n",
       "      <td>1.283986</td>\n",
       "      <td>3.573711</td>\n",
       "      <td>5.467160</td>\n",
       "      <td>26.989333</td>\n",
       "      <td>2.208898</td>\n",
       "      <td>4.421074</td>\n",
       "      <td>4.876906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>35</td>\n",
       "      <td>control_20</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>628.590237</td>\n",
       "      <td>360.882931</td>\n",
       "      <td>1.340389</td>\n",
       "      <td>...</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>28.174359</td>\n",
       "      <td>78.590739</td>\n",
       "      <td>2.789442</td>\n",
       "      <td>15.156166</td>\n",
       "      <td>5.075426</td>\n",
       "      <td>31.806338</td>\n",
       "      <td>2.290338</td>\n",
       "      <td>4.069139</td>\n",
       "      <td>4.453520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9</td>\n",
       "      <td>condition_18</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>247.227032</td>\n",
       "      <td>73.876085</td>\n",
       "      <td>1.842504</td>\n",
       "      <td>...</td>\n",
       "      <td>13.384615</td>\n",
       "      <td>5.724138</td>\n",
       "      <td>4.337469</td>\n",
       "      <td>0.757751</td>\n",
       "      <td>2.710631</td>\n",
       "      <td>4.955757</td>\n",
       "      <td>39.456842</td>\n",
       "      <td>1.993108</td>\n",
       "      <td>10.273531</td>\n",
       "      <td>5.437538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46</td>\n",
       "      <td>control_30</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>479.068105</td>\n",
       "      <td>211.548094</td>\n",
       "      <td>1.588177</td>\n",
       "      <td>...</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>13.044444</td>\n",
       "      <td>15.014577</td>\n",
       "      <td>1.151032</td>\n",
       "      <td>3.039469</td>\n",
       "      <td>5.313793</td>\n",
       "      <td>30.839623</td>\n",
       "      <td>3.024060</td>\n",
       "      <td>9.120444</td>\n",
       "      <td>4.483430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22</td>\n",
       "      <td>condition_9</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>524.559342</td>\n",
       "      <td>178.572499</td>\n",
       "      <td>1.719120</td>\n",
       "      <td>...</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>10.411765</td>\n",
       "      <td>10.342114</td>\n",
       "      <td>0.993310</td>\n",
       "      <td>3.657695</td>\n",
       "      <td>5.757911</td>\n",
       "      <td>25.872047</td>\n",
       "      <td>1.830192</td>\n",
       "      <td>3.913150</td>\n",
       "      <td>5.372943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15</td>\n",
       "      <td>condition_23</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>548.367255</td>\n",
       "      <td>252.873974</td>\n",
       "      <td>1.512638</td>\n",
       "      <td>...</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>19.686364</td>\n",
       "      <td>24.952379</td>\n",
       "      <td>1.267496</td>\n",
       "      <td>3.131237</td>\n",
       "      <td>5.557883</td>\n",
       "      <td>36.390181</td>\n",
       "      <td>1.866499</td>\n",
       "      <td>3.132741</td>\n",
       "      <td>4.978360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>condition_16</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>586.106884</td>\n",
       "      <td>242.076106</td>\n",
       "      <td>1.601059</td>\n",
       "      <td>...</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>24.448034</td>\n",
       "      <td>1.270028</td>\n",
       "      <td>2.698337</td>\n",
       "      <td>5.464818</td>\n",
       "      <td>39.954424</td>\n",
       "      <td>2.501441</td>\n",
       "      <td>4.168626</td>\n",
       "      <td>4.643154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>control_27</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>608.803595</td>\n",
       "      <td>314.724490</td>\n",
       "      <td>1.377708</td>\n",
       "      <td>...</td>\n",
       "      <td>35.538462</td>\n",
       "      <td>17.149351</td>\n",
       "      <td>20.769612</td>\n",
       "      <td>1.211102</td>\n",
       "      <td>4.066230</td>\n",
       "      <td>5.669566</td>\n",
       "      <td>24.430233</td>\n",
       "      <td>2.642503</td>\n",
       "      <td>4.887923</td>\n",
       "      <td>4.811409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>44</td>\n",
       "      <td>control_29</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>584.625886</td>\n",
       "      <td>285.417779</td>\n",
       "      <td>1.434045</td>\n",
       "      <td>...</td>\n",
       "      <td>39.538462</td>\n",
       "      <td>14.961089</td>\n",
       "      <td>17.060106</td>\n",
       "      <td>1.140298</td>\n",
       "      <td>2.778698</td>\n",
       "      <td>5.790023</td>\n",
       "      <td>23.037445</td>\n",
       "      <td>2.354025</td>\n",
       "      <td>5.062840</td>\n",
       "      <td>5.046790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>control_23</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>409.365576</td>\n",
       "      <td>198.761298</td>\n",
       "      <td>1.459301</td>\n",
       "      <td>...</td>\n",
       "      <td>36.076923</td>\n",
       "      <td>15.132196</td>\n",
       "      <td>66.697169</td>\n",
       "      <td>4.407633</td>\n",
       "      <td>20.840382</td>\n",
       "      <td>5.163827</td>\n",
       "      <td>26.663755</td>\n",
       "      <td>2.796827</td>\n",
       "      <td>5.597143</td>\n",
       "      <td>4.836905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>control_19</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>497.025070</td>\n",
       "      <td>226.506920</td>\n",
       "      <td>1.504780</td>\n",
       "      <td>...</td>\n",
       "      <td>36.384615</td>\n",
       "      <td>13.955603</td>\n",
       "      <td>15.838704</td>\n",
       "      <td>1.134935</td>\n",
       "      <td>3.636279</td>\n",
       "      <td>5.738435</td>\n",
       "      <td>28.545679</td>\n",
       "      <td>2.372276</td>\n",
       "      <td>4.508271</td>\n",
       "      <td>4.832628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>28</td>\n",
       "      <td>control_14</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>857.646813</td>\n",
       "      <td>372.680639</td>\n",
       "      <td>1.530664</td>\n",
       "      <td>...</td>\n",
       "      <td>32.307692</td>\n",
       "      <td>18.040476</td>\n",
       "      <td>24.550290</td>\n",
       "      <td>1.360845</td>\n",
       "      <td>3.810761</td>\n",
       "      <td>5.474782</td>\n",
       "      <td>25.420824</td>\n",
       "      <td>2.888564</td>\n",
       "      <td>15.777664</td>\n",
       "      <td>5.163923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>24</td>\n",
       "      <td>control_10</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>646.649079</td>\n",
       "      <td>268.098850</td>\n",
       "      <td>1.597867</td>\n",
       "      <td>...</td>\n",
       "      <td>30.375000</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>94.571893</td>\n",
       "      <td>3.995995</td>\n",
       "      <td>13.981616</td>\n",
       "      <td>4.211763</td>\n",
       "      <td>36.158654</td>\n",
       "      <td>3.426673</td>\n",
       "      <td>7.558278</td>\n",
       "      <td>3.770760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>34</td>\n",
       "      <td>control_2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>663.349094</td>\n",
       "      <td>414.694765</td>\n",
       "      <td>1.298569</td>\n",
       "      <td>...</td>\n",
       "      <td>29.200000</td>\n",
       "      <td>24.544521</td>\n",
       "      <td>35.708772</td>\n",
       "      <td>1.454857</td>\n",
       "      <td>3.536560</td>\n",
       "      <td>5.730021</td>\n",
       "      <td>37.305556</td>\n",
       "      <td>2.912125</td>\n",
       "      <td>5.720875</td>\n",
       "      <td>4.464211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>control_18</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>547.782723</td>\n",
       "      <td>294.271442</td>\n",
       "      <td>1.375090</td>\n",
       "      <td>...</td>\n",
       "      <td>38.769231</td>\n",
       "      <td>16.093254</td>\n",
       "      <td>16.213573</td>\n",
       "      <td>1.007476</td>\n",
       "      <td>2.522372</td>\n",
       "      <td>5.844471</td>\n",
       "      <td>30.966851</td>\n",
       "      <td>3.478652</td>\n",
       "      <td>7.969285</td>\n",
       "      <td>4.288265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>52</td>\n",
       "      <td>control_7</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>930.054446</td>\n",
       "      <td>406.857136</td>\n",
       "      <td>1.569004</td>\n",
       "      <td>...</td>\n",
       "      <td>33.692308</td>\n",
       "      <td>21.981735</td>\n",
       "      <td>54.756416</td>\n",
       "      <td>2.490996</td>\n",
       "      <td>15.202120</td>\n",
       "      <td>5.301267</td>\n",
       "      <td>26.867606</td>\n",
       "      <td>2.934253</td>\n",
       "      <td>4.733967</td>\n",
       "      <td>4.427333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>41</td>\n",
       "      <td>control_26</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>572.449530</td>\n",
       "      <td>360.169651</td>\n",
       "      <td>1.298383</td>\n",
       "      <td>...</td>\n",
       "      <td>33.538462</td>\n",
       "      <td>22.211009</td>\n",
       "      <td>25.373467</td>\n",
       "      <td>1.142382</td>\n",
       "      <td>2.667172</td>\n",
       "      <td>5.599076</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.753269</td>\n",
       "      <td>4.487373</td>\n",
       "      <td>4.199486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50</td>\n",
       "      <td>control_5</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>537.767967</td>\n",
       "      <td>306.566695</td>\n",
       "      <td>1.314363</td>\n",
       "      <td>...</td>\n",
       "      <td>33.769231</td>\n",
       "      <td>19.653759</td>\n",
       "      <td>22.798270</td>\n",
       "      <td>1.159995</td>\n",
       "      <td>2.350495</td>\n",
       "      <td>5.587637</td>\n",
       "      <td>28.049419</td>\n",
       "      <td>2.383390</td>\n",
       "      <td>4.043696</td>\n",
       "      <td>4.609746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27</td>\n",
       "      <td>control_13</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>436.083353</td>\n",
       "      <td>180.464606</td>\n",
       "      <td>1.615202</td>\n",
       "      <td>...</td>\n",
       "      <td>31.692308</td>\n",
       "      <td>12.305825</td>\n",
       "      <td>13.360932</td>\n",
       "      <td>1.085740</td>\n",
       "      <td>3.412288</td>\n",
       "      <td>5.630121</td>\n",
       "      <td>33.952719</td>\n",
       "      <td>2.876796</td>\n",
       "      <td>8.705948</td>\n",
       "      <td>4.783905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>control_16</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>469.933772</td>\n",
       "      <td>245.432164</td>\n",
       "      <td>1.406363</td>\n",
       "      <td>...</td>\n",
       "      <td>40.384615</td>\n",
       "      <td>16.592381</td>\n",
       "      <td>63.798926</td>\n",
       "      <td>3.845074</td>\n",
       "      <td>21.204787</td>\n",
       "      <td>5.338755</td>\n",
       "      <td>21.501031</td>\n",
       "      <td>2.589488</td>\n",
       "      <td>5.319297</td>\n",
       "      <td>5.021345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>condition_10</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>580.711403</td>\n",
       "      <td>321.150497</td>\n",
       "      <td>1.348230</td>\n",
       "      <td>...</td>\n",
       "      <td>27.222222</td>\n",
       "      <td>25.906122</td>\n",
       "      <td>44.416750</td>\n",
       "      <td>1.714527</td>\n",
       "      <td>7.763403</td>\n",
       "      <td>4.841926</td>\n",
       "      <td>38.792350</td>\n",
       "      <td>2.208803</td>\n",
       "      <td>3.599703</td>\n",
       "      <td>4.043482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3</td>\n",
       "      <td>condition_12</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>433.170487</td>\n",
       "      <td>174.285592</td>\n",
       "      <td>1.648595</td>\n",
       "      <td>...</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>11.515152</td>\n",
       "      <td>12.516208</td>\n",
       "      <td>1.086934</td>\n",
       "      <td>3.300979</td>\n",
       "      <td>5.592052</td>\n",
       "      <td>28.011494</td>\n",
       "      <td>2.216177</td>\n",
       "      <td>4.359810</td>\n",
       "      <td>5.026175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>control_12</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>341.117566</td>\n",
       "      <td>147.202370</td>\n",
       "      <td>1.597837</td>\n",
       "      <td>...</td>\n",
       "      <td>28.285714</td>\n",
       "      <td>12.378788</td>\n",
       "      <td>14.921837</td>\n",
       "      <td>1.205436</td>\n",
       "      <td>4.124580</td>\n",
       "      <td>5.541758</td>\n",
       "      <td>36.585082</td>\n",
       "      <td>2.519433</td>\n",
       "      <td>9.393374</td>\n",
       "      <td>4.984534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>control_3</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>450.847169</td>\n",
       "      <td>251.233880</td>\n",
       "      <td>1.409631</td>\n",
       "      <td>...</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>16.975000</td>\n",
       "      <td>17.689084</td>\n",
       "      <td>1.042067</td>\n",
       "      <td>2.667753</td>\n",
       "      <td>5.687968</td>\n",
       "      <td>31.866667</td>\n",
       "      <td>3.549919</td>\n",
       "      <td>7.853831</td>\n",
       "      <td>4.095484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>19</td>\n",
       "      <td>condition_6</td>\n",
       "      <td>Mild</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>572.615994</td>\n",
       "      <td>194.747287</td>\n",
       "      <td>1.736059</td>\n",
       "      <td>...</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>12.099206</td>\n",
       "      <td>15.125573</td>\n",
       "      <td>1.250129</td>\n",
       "      <td>5.264818</td>\n",
       "      <td>5.076233</td>\n",
       "      <td>30.568465</td>\n",
       "      <td>2.772494</td>\n",
       "      <td>7.030579</td>\n",
       "      <td>4.259779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>23</td>\n",
       "      <td>control_1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>575.290244</td>\n",
       "      <td>226.214831</td>\n",
       "      <td>1.582961</td>\n",
       "      <td>...</td>\n",
       "      <td>28.750000</td>\n",
       "      <td>22.569565</td>\n",
       "      <td>108.942683</td>\n",
       "      <td>4.826973</td>\n",
       "      <td>11.397110</td>\n",
       "      <td>3.786479</td>\n",
       "      <td>31.007722</td>\n",
       "      <td>2.142550</td>\n",
       "      <td>4.813988</td>\n",
       "      <td>4.583174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0        number madrs2_grouped  madrs2_grouped_ord  Normal  Mild  \\\n",
       "45          36    control_21         Normal                   0       1     0   \n",
       "33          13  condition_21       Moderate                   2       0     0   \n",
       "40          12  condition_20       Moderate                   2       0     0   \n",
       "26           5  condition_14           Mild                   1       0     1   \n",
       "11          10  condition_19       Moderate                   2       0     0   \n",
       "2           40    control_25         Normal                   0       1     0   \n",
       "32           8  condition_17           Mild                   1       0     1   \n",
       "43           6  condition_15           Mild                   1       0     1   \n",
       "46          21   condition_8           Mild                   1       0     1   \n",
       "30          20   condition_7       Moderate                   2       0     0   \n",
       "4           11   condition_2           Mild                   1       0     1   \n",
       "10           4  condition_13           Mild                   1       0     1   \n",
       "28          16   condition_3       Moderate                   2       0     0   \n",
       "22          18   condition_5       Moderate                   2       0     0   \n",
       "31          48    control_32         Normal                   0       1     0   \n",
       "49          39    control_24         Normal                   0       1     0   \n",
       "37          54     control_9         Normal                   0       1     0   \n",
       "7           43    control_28         Normal                   0       1     0   \n",
       "14          31    control_17         Normal                   0       1     0   \n",
       "27          29    control_15         Normal                   0       1     0   \n",
       "35          17   condition_4           Mild                   1       0     1   \n",
       "50          51     control_6         Normal                   0       1     0   \n",
       "18          14  condition_22       Moderate                   2       0     0   \n",
       "52           0   condition_1           Mild                   1       0     1   \n",
       "34          25    control_11         Normal                   0       1     0   \n",
       "15          49     control_4         Normal                   0       1     0   \n",
       "5            2  condition_11       Moderate                   2       0     0   \n",
       "29          53     control_8         Normal                   0       1     0   \n",
       "16          37    control_22         Normal                   0       1     0   \n",
       "53          47    control_31         Normal                   0       1     0   \n",
       "20          35    control_20         Normal                   0       1     0   \n",
       "48           9  condition_18           Mild                   1       0     1   \n",
       "8           46    control_30         Normal                   0       1     0   \n",
       "13          22   condition_9       Moderate                   2       0     0   \n",
       "25          15  condition_23       Moderate                   2       0     0   \n",
       "17           7  condition_16           Mild                   1       0     1   \n",
       "41          42    control_27         Normal                   0       1     0   \n",
       "54          44    control_29         Normal                   0       1     0   \n",
       "38          38    control_23         Normal                   0       1     0   \n",
       "1           33    control_19         Normal                   0       1     0   \n",
       "12          28    control_14         Normal                   0       1     0   \n",
       "42          24    control_10         Normal                   0       1     0   \n",
       "24          34     control_2         Normal                   0       1     0   \n",
       "6           32    control_18         Normal                   0       1     0   \n",
       "23          52     control_7         Normal                   0       1     0   \n",
       "36          41    control_26         Normal                   0       1     0   \n",
       "21          50     control_5         Normal                   0       1     0   \n",
       "19          27    control_13         Normal                   0       1     0   \n",
       "9           30    control_16         Normal                   0       1     0   \n",
       "39           1  condition_10       Moderate                   2       0     0   \n",
       "51           3  condition_12       Moderate                   2       0     0   \n",
       "3           26    control_12         Normal                   0       1     0   \n",
       "0           45     control_3         Normal                   0       1     0   \n",
       "47          19   condition_6           Mild                   1       0     1   \n",
       "44          23     control_1         Normal                   0       1     0   \n",
       "\n",
       "    Moderate  activity_intraday_variability  activity_mean  \\\n",
       "45         0                     501.256833     264.990354   \n",
       "33         1                     581.136493      79.936852   \n",
       "40         1                     375.133788      69.371695   \n",
       "26         0                     415.016644      75.205611   \n",
       "11         1                     514.936507     165.284240   \n",
       "2          0                     578.504177     316.378243   \n",
       "32         0                     280.946142      86.749194   \n",
       "43         0                     446.762140     117.579443   \n",
       "46         0                     702.131570     218.154069   \n",
       "30         1                     651.798586     322.489322   \n",
       "4          0                     566.183307     211.694251   \n",
       "10         0                     487.534039     262.156813   \n",
       "28         1                     538.448261     266.215983   \n",
       "22         1                     479.816472     173.693090   \n",
       "31         0                     564.941965     160.257003   \n",
       "49         0                     958.001261     377.208889   \n",
       "37         0                     456.019285     138.380029   \n",
       "7          0                     561.622641     293.018097   \n",
       "14         0                     463.476496     240.285020   \n",
       "27         0                     602.243602     295.178077   \n",
       "35         0                     578.098625     291.488963   \n",
       "50         0                     717.492149     346.517833   \n",
       "18         1                     396.144454     162.486041   \n",
       "52         0                     429.453604     155.345969   \n",
       "34         0                     551.650117     193.321593   \n",
       "15         0                     714.953073     207.433702   \n",
       "5          1                     542.382958     132.735775   \n",
       "29         0                     595.110233     407.068605   \n",
       "16         0                     560.200512     257.622303   \n",
       "53         0                     795.174523     367.441441   \n",
       "20         0                     628.590237     360.882931   \n",
       "48         0                     247.227032      73.876085   \n",
       "8          0                     479.068105     211.548094   \n",
       "13         1                     524.559342     178.572499   \n",
       "25         1                     548.367255     252.873974   \n",
       "17         0                     586.106884     242.076106   \n",
       "41         0                     608.803595     314.724490   \n",
       "54         0                     584.625886     285.417779   \n",
       "38         0                     409.365576     198.761298   \n",
       "1          0                     497.025070     226.506920   \n",
       "12         0                     857.646813     372.680639   \n",
       "42         0                     646.649079     268.098850   \n",
       "24         0                     663.349094     414.694765   \n",
       "6          0                     547.782723     294.271442   \n",
       "23         0                     930.054446     406.857136   \n",
       "36         0                     572.449530     360.169651   \n",
       "21         0                     537.767967     306.566695   \n",
       "19         0                     436.083353     180.464606   \n",
       "9          0                     469.933772     245.432164   \n",
       "39         1                     580.711403     321.150497   \n",
       "51         1                     433.170487     174.285592   \n",
       "3          0                     341.117566     147.202370   \n",
       "0          0                     450.847169     251.233880   \n",
       "47         0                     572.615994     194.747287   \n",
       "44         0                     575.290244     226.214831   \n",
       "\n",
       "    activity_cv_minutely  ...  act_bout_num  act_bout_dur_mean  \\\n",
       "45              1.390468  ...     38.750000          15.570968   \n",
       "33              2.697304  ...     17.923077           9.236052   \n",
       "40              2.351840  ...     16.153846           7.628571   \n",
       "26              2.487839  ...     19.000000           7.894737   \n",
       "11              1.766114  ...     33.461538          10.245977   \n",
       "2               1.353900  ...     30.461538          25.111111   \n",
       "32              1.795802  ...     22.692308          11.037288   \n",
       "43              2.027575  ...     23.769231          10.097087   \n",
       "46              1.942973  ...     31.400000          14.133758   \n",
       "30              1.511731  ...     26.000000          21.104895   \n",
       "4               1.773271  ...     32.055556          15.303293   \n",
       "10              1.385956  ...     41.928571          13.521295   \n",
       "28              1.499762  ...     30.769231          20.410000   \n",
       "22              1.731070  ...     27.153846          13.059490   \n",
       "31              1.853685  ...     24.785714          12.720461   \n",
       "49              1.700495  ...     32.692308          20.138824   \n",
       "37              1.899449  ...     26.384615          11.075802   \n",
       "7               1.394358  ...     47.500000          14.250000   \n",
       "14              1.406504  ...     40.111111          14.160665   \n",
       "27              1.415641  ...     31.000000          20.903226   \n",
       "35              1.559481  ...     31.846154          16.789855   \n",
       "50              1.443704  ...     38.307692          16.128514   \n",
       "18              1.563656  ...     34.642857          14.377320   \n",
       "52              1.761592  ...     33.000000          13.479339   \n",
       "34              1.730879  ...     34.153846          14.506757   \n",
       "15              1.880670  ...     30.538462          11.528967   \n",
       "5               2.113054  ...     23.571429          10.981818   \n",
       "29              1.222877  ...     32.923077          28.392523   \n",
       "16              1.500055  ...     37.615385          14.312883   \n",
       "53              1.483490  ...     31.230769          20.581281   \n",
       "20              1.340389  ...     30.000000          28.174359   \n",
       "48              1.842504  ...     13.384615           5.724138   \n",
       "8               1.588177  ...     35.000000          13.044444   \n",
       "13              1.719120  ...     34.000000          10.411765   \n",
       "25              1.512638  ...     27.500000          19.686364   \n",
       "17              1.601059  ...     25.500000          19.250000   \n",
       "41              1.377708  ...     35.538462          17.149351   \n",
       "54              1.434045  ...     39.538462          14.961089   \n",
       "38              1.459301  ...     36.076923          15.132196   \n",
       "1               1.504780  ...     36.384615          13.955603   \n",
       "12              1.530664  ...     32.307692          18.040476   \n",
       "42              1.597867  ...     30.375000          23.666667   \n",
       "24              1.298569  ...     29.200000          24.544521   \n",
       "6               1.375090  ...     38.769231          16.093254   \n",
       "23              1.569004  ...     33.692308          21.981735   \n",
       "36              1.298383  ...     33.538462          22.211009   \n",
       "21              1.314363  ...     33.769231          19.653759   \n",
       "19              1.615202  ...     31.692308          12.305825   \n",
       "9               1.406363  ...     40.384615          16.592381   \n",
       "39              1.348230  ...     27.222222          25.906122   \n",
       "51              1.648595  ...     33.000000          11.515152   \n",
       "3               1.597837  ...     28.285714          12.378788   \n",
       "0               1.409631  ...     36.666667          16.975000   \n",
       "47              1.736059  ...     36.000000          12.099206   \n",
       "44              1.582961  ...     28.750000          22.569565   \n",
       "\n",
       "    act_bout_dur_std  act_bout_dur_cv  act_bout_dur_skew  \\\n",
       "45         20.479364         1.315227           5.337884   \n",
       "33          9.194736         0.995527           2.322436   \n",
       "40          5.674179         0.743806           2.332610   \n",
       "26          7.177810         0.909189           4.969473   \n",
       "11         11.835146         1.155102           3.084005   \n",
       "2          75.382117         3.001943          16.823950   \n",
       "32         72.976806         6.611842          17.084481   \n",
       "43         10.989234         1.088357           3.596046   \n",
       "46         18.433907         1.304247           2.852017   \n",
       "30         50.033913         2.370726           5.344657   \n",
       "4          61.551835         4.022130          21.570781   \n",
       "10         15.757683         1.165398           3.217821   \n",
       "28         66.217133         3.244348          17.668665   \n",
       "22         17.089150         1.308562           3.092334   \n",
       "31         12.421300         0.976482           2.539530   \n",
       "49         27.367382         1.358936           4.123580   \n",
       "37         10.882148         0.982516           3.207440   \n",
       "7          53.555784         3.758301          24.874405   \n",
       "14         24.128745         1.703927          10.282041   \n",
       "27         28.050994         1.341946           4.082569   \n",
       "35         29.264376         1.742980           5.130374   \n",
       "50         20.642681         1.279887           3.562979   \n",
       "18         59.781730         4.158058          20.792508   \n",
       "52         66.599417         4.940852          18.155197   \n",
       "34         68.969666         4.754313          19.954122   \n",
       "15         11.929392         1.034732           3.208551   \n",
       "5           9.170107         0.835026           1.772614   \n",
       "29         74.975001         2.640660          15.835934   \n",
       "16         17.048696         1.191143           4.580120   \n",
       "53         26.426082         1.283986           3.573711   \n",
       "20         78.590739         2.789442          15.156166   \n",
       "48          4.337469         0.757751           2.710631   \n",
       "8          15.014577         1.151032           3.039469   \n",
       "13         10.342114         0.993310           3.657695   \n",
       "25         24.952379         1.267496           3.131237   \n",
       "17         24.448034         1.270028           2.698337   \n",
       "41         20.769612         1.211102           4.066230   \n",
       "54         17.060106         1.140298           2.778698   \n",
       "38         66.697169         4.407633          20.840382   \n",
       "1          15.838704         1.134935           3.636279   \n",
       "12         24.550290         1.360845           3.810761   \n",
       "42         94.571893         3.995995          13.981616   \n",
       "24         35.708772         1.454857           3.536560   \n",
       "6          16.213573         1.007476           2.522372   \n",
       "23         54.756416         2.490996          15.202120   \n",
       "36         25.373467         1.142382           2.667172   \n",
       "21         22.798270         1.159995           2.350495   \n",
       "19         13.360932         1.085740           3.412288   \n",
       "9          63.798926         3.845074          21.204787   \n",
       "39         44.416750         1.714527           7.763403   \n",
       "51         12.516208         1.086934           3.300979   \n",
       "3          14.921837         1.205436           4.124580   \n",
       "0          17.689084         1.042067           2.667753   \n",
       "47         15.125573         1.250129           5.264818   \n",
       "44        108.942683         4.826973          11.397110   \n",
       "\n",
       "    act_bout_dur_entropy  inact_bout_dur_mean  inact_bout_dur_cv  \\\n",
       "45              5.238865            21.813880           2.368584   \n",
       "33              5.096657            64.707143           2.424348   \n",
       "40              5.132767            46.642857           1.954293   \n",
       "26              5.324928            47.501312           1.585688   \n",
       "11              5.645285            29.305439           2.279229   \n",
       "2               5.101094            29.782738           2.313598   \n",
       "32              4.050248            39.730310           2.243364   \n",
       "43              5.359727            38.069307           2.371731   \n",
       "46              4.502515            39.200000           1.957940   \n",
       "30              4.613154            36.039735           3.064516   \n",
       "4               5.341824            34.537698           2.350872   \n",
       "10              5.918785            25.356415           3.116847   \n",
       "28              5.103950            32.049587           2.002618   \n",
       "22              5.332214            32.864286           1.959681   \n",
       "31              5.489731            42.167116           2.566225   \n",
       "49              5.508435            32.023810           3.579543   \n",
       "37              5.498305            40.630380           2.853196   \n",
       "7               5.783849            16.941345           1.810419   \n",
       "14              5.340840            24.403125           2.448441   \n",
       "27              5.291063            39.979757           2.904364   \n",
       "35              5.306521            25.475845           2.560030   \n",
       "50              5.698802            24.259542           2.804492   \n",
       "18              5.218089            28.060120           1.901712   \n",
       "52              4.664714            30.368286           2.586638   \n",
       "34              4.928668            27.261411           2.264217   \n",
       "15              5.617613            26.201161           2.118761   \n",
       "5               5.514034            45.690608           2.469748   \n",
       "29              5.242367            31.210317           2.289926   \n",
       "16              5.744807            25.157407           2.585875   \n",
       "53              5.467160            26.989333           2.208898   \n",
       "20              5.075426            31.806338           2.290338   \n",
       "48              4.955757            39.456842           1.993108   \n",
       "8               5.313793            30.839623           3.024060   \n",
       "13              5.757911            25.872047           1.830192   \n",
       "25              5.557883            36.390181           1.866499   \n",
       "17              5.464818            39.954424           2.501441   \n",
       "41              5.669566            24.430233           2.642503   \n",
       "54              5.790023            23.037445           2.354025   \n",
       "38              5.163827            26.663755           2.796827   \n",
       "1               5.738435            28.545679           2.372276   \n",
       "12              5.474782            25.420824           2.888564   \n",
       "42              4.211763            36.158654           3.426673   \n",
       "24              5.730021            37.305556           2.912125   \n",
       "6               5.844471            30.966851           3.478652   \n",
       "23              5.301267            26.867606           2.934253   \n",
       "36              5.599076            32.000000           2.753269   \n",
       "21              5.587637            28.049419           2.383390   \n",
       "19              5.630121            33.952719           2.876796   \n",
       "9               5.338755            21.501031           2.589488   \n",
       "39              4.841926            38.792350           2.208803   \n",
       "51              5.592052            28.011494           2.216177   \n",
       "3               5.541758            36.585082           2.519433   \n",
       "0               5.687968            31.866667           3.549919   \n",
       "47              5.076233            30.568465           2.772494   \n",
       "44              3.786479            31.007722           2.142550   \n",
       "\n",
       "    inact_bout_dur_skew  inact_bout_dur_entropy  \n",
       "45             5.583540                4.729848  \n",
       "33             4.328888                4.389412  \n",
       "40             4.103701                5.017741  \n",
       "26             5.739879                5.259853  \n",
       "11             4.874779                5.136790  \n",
       "2              4.512948                4.707541  \n",
       "32             4.879217                5.030853  \n",
       "43             4.677363                4.865471  \n",
       "46             3.703643                4.016461  \n",
       "30             8.197120                4.311731  \n",
       "4              4.200135                4.979392  \n",
       "10            10.843546                4.908043  \n",
       "28             4.440771                4.918294  \n",
       "22             4.490926                5.170926  \n",
       "31             4.999590                4.698126  \n",
       "49             7.418587                4.111767  \n",
       "37             8.598373                4.759101  \n",
       "7              4.703947                5.797800  \n",
       "14             5.119227                4.657088  \n",
       "27             8.091174                4.159221  \n",
       "35             5.353134                4.880975  \n",
       "50             5.472237                4.658652  \n",
       "18             3.418467                5.275094  \n",
       "52             5.029551                4.747316  \n",
       "34             5.110121                5.149185  \n",
       "15             4.954406                5.286521  \n",
       "5              4.516316                4.637721  \n",
       "29             3.721090                4.328408  \n",
       "16             5.206024                4.884087  \n",
       "53             4.421074                4.876906  \n",
       "20             4.069139                4.453520  \n",
       "48            10.273531                5.437538  \n",
       "8              9.120444                4.483430  \n",
       "13             3.913150                5.372943  \n",
       "25             3.132741                4.978360  \n",
       "17             4.168626                4.643154  \n",
       "41             4.887923                4.811409  \n",
       "54             5.062840                5.046790  \n",
       "38             5.597143                4.836905  \n",
       "1              4.508271                4.832628  \n",
       "12            15.777664                5.163923  \n",
       "42             7.558278                3.770760  \n",
       "24             5.720875                4.464211  \n",
       "6              7.969285                4.288265  \n",
       "23             4.733967                4.427333  \n",
       "36             4.487373                4.199486  \n",
       "21             4.043696                4.609746  \n",
       "19             8.705948                4.783905  \n",
       "9              5.319297                5.021345  \n",
       "39             3.599703                4.043482  \n",
       "51             4.359810                5.026175  \n",
       "3              9.393374                4.984534  \n",
       "0              7.853831                4.095484  \n",
       "47             7.030579                4.259779  \n",
       "44             4.813988                4.583174  \n",
       "\n",
       "[55 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#extract indices\n",
    "indices = np.arange(df_features.shape[0])\n",
    "#shuffle indices\n",
    "np.random.seed(0)\n",
    "shuffled_indices = np.random.permutation(indices)\n",
    "#reindex the shuffle indices\n",
    "df_features = df_features.reindex(shuffled_indices)\n",
    "display(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>madrs2_grouped</th>\n",
       "      <th>activity_intraday_variability</th>\n",
       "      <th>activity_mean</th>\n",
       "      <th>activity_cv_minutely</th>\n",
       "      <th>activity_skewness_hourly</th>\n",
       "      <th>activity_entropy_hourly</th>\n",
       "      <th>activity_pct_0to6</th>\n",
       "      <th>activity_pct_6to12</th>\n",
       "      <th>activity_pct_12to18</th>\n",
       "      <th>...</th>\n",
       "      <th>act_bout_num</th>\n",
       "      <th>act_bout_dur_mean</th>\n",
       "      <th>act_bout_dur_std</th>\n",
       "      <th>act_bout_dur_cv</th>\n",
       "      <th>act_bout_dur_skew</th>\n",
       "      <th>act_bout_dur_entropy</th>\n",
       "      <th>inact_bout_dur_mean</th>\n",
       "      <th>inact_bout_dur_cv</th>\n",
       "      <th>inact_bout_dur_skew</th>\n",
       "      <th>inact_bout_dur_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>36</td>\n",
       "      <td>Normal</td>\n",
       "      <td>501.256833</td>\n",
       "      <td>264.990354</td>\n",
       "      <td>1.390468</td>\n",
       "      <td>2.319655</td>\n",
       "      <td>3.017930</td>\n",
       "      <td>0.285579</td>\n",
       "      <td>0.225209</td>\n",
       "      <td>0.349568</td>\n",
       "      <td>...</td>\n",
       "      <td>38.750000</td>\n",
       "      <td>15.570968</td>\n",
       "      <td>20.479364</td>\n",
       "      <td>1.315227</td>\n",
       "      <td>5.337884</td>\n",
       "      <td>5.238865</td>\n",
       "      <td>21.813880</td>\n",
       "      <td>2.368584</td>\n",
       "      <td>5.583540</td>\n",
       "      <td>4.729848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>13</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>581.136493</td>\n",
       "      <td>79.936852</td>\n",
       "      <td>2.697304</td>\n",
       "      <td>3.787372</td>\n",
       "      <td>2.147124</td>\n",
       "      <td>0.025093</td>\n",
       "      <td>0.400688</td>\n",
       "      <td>0.444788</td>\n",
       "      <td>...</td>\n",
       "      <td>17.923077</td>\n",
       "      <td>9.236052</td>\n",
       "      <td>9.194736</td>\n",
       "      <td>0.995527</td>\n",
       "      <td>2.322436</td>\n",
       "      <td>5.096657</td>\n",
       "      <td>64.707143</td>\n",
       "      <td>2.424348</td>\n",
       "      <td>4.328888</td>\n",
       "      <td>4.389412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>12</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>375.133788</td>\n",
       "      <td>69.371695</td>\n",
       "      <td>2.351840</td>\n",
       "      <td>3.688455</td>\n",
       "      <td>2.148713</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.343435</td>\n",
       "      <td>0.420807</td>\n",
       "      <td>...</td>\n",
       "      <td>16.153846</td>\n",
       "      <td>7.628571</td>\n",
       "      <td>5.674179</td>\n",
       "      <td>0.743806</td>\n",
       "      <td>2.332610</td>\n",
       "      <td>5.132767</td>\n",
       "      <td>46.642857</td>\n",
       "      <td>1.954293</td>\n",
       "      <td>4.103701</td>\n",
       "      <td>5.017741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>Mild</td>\n",
       "      <td>415.016644</td>\n",
       "      <td>75.205611</td>\n",
       "      <td>2.487839</td>\n",
       "      <td>3.412524</td>\n",
       "      <td>2.284191</td>\n",
       "      <td>0.311458</td>\n",
       "      <td>0.532379</td>\n",
       "      <td>0.269847</td>\n",
       "      <td>...</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>7.894737</td>\n",
       "      <td>7.177810</td>\n",
       "      <td>0.909189</td>\n",
       "      <td>4.969473</td>\n",
       "      <td>5.324928</td>\n",
       "      <td>47.501312</td>\n",
       "      <td>1.585688</td>\n",
       "      <td>5.739879</td>\n",
       "      <td>5.259853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>514.936507</td>\n",
       "      <td>165.284240</td>\n",
       "      <td>1.766114</td>\n",
       "      <td>2.869305</td>\n",
       "      <td>2.801761</td>\n",
       "      <td>0.043608</td>\n",
       "      <td>0.348435</td>\n",
       "      <td>0.451102</td>\n",
       "      <td>...</td>\n",
       "      <td>33.461538</td>\n",
       "      <td>10.245977</td>\n",
       "      <td>11.835146</td>\n",
       "      <td>1.155102</td>\n",
       "      <td>3.084005</td>\n",
       "      <td>5.645285</td>\n",
       "      <td>29.305439</td>\n",
       "      <td>2.279229</td>\n",
       "      <td>4.874779</td>\n",
       "      <td>5.136790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 madrs2_grouped  activity_intraday_variability  activity_mean  \\\n",
       "45          36         Normal                     501.256833     264.990354   \n",
       "33          13       Moderate                     581.136493      79.936852   \n",
       "40          12       Moderate                     375.133788      69.371695   \n",
       "26           5           Mild                     415.016644      75.205611   \n",
       "11          10       Moderate                     514.936507     165.284240   \n",
       "\n",
       "    activity_cv_minutely  activity_skewness_hourly  activity_entropy_hourly  \\\n",
       "45              1.390468                  2.319655                 3.017930   \n",
       "33              2.697304                  3.787372                 2.147124   \n",
       "40              2.351840                  3.688455                 2.148713   \n",
       "26              2.487839                  3.412524                 2.284191   \n",
       "11              1.766114                  2.869305                 2.801761   \n",
       "\n",
       "    activity_pct_0to6  activity_pct_6to12  activity_pct_12to18  ...  \\\n",
       "45           0.285579            0.225209             0.349568  ...   \n",
       "33           0.025093            0.400688             0.444788  ...   \n",
       "40           0.055103            0.343435             0.420807  ...   \n",
       "26           0.311458            0.532379             0.269847  ...   \n",
       "11           0.043608            0.348435             0.451102  ...   \n",
       "\n",
       "    act_bout_num  act_bout_dur_mean  act_bout_dur_std  act_bout_dur_cv  \\\n",
       "45     38.750000          15.570968         20.479364         1.315227   \n",
       "33     17.923077           9.236052          9.194736         0.995527   \n",
       "40     16.153846           7.628571          5.674179         0.743806   \n",
       "26     19.000000           7.894737          7.177810         0.909189   \n",
       "11     33.461538          10.245977         11.835146         1.155102   \n",
       "\n",
       "    act_bout_dur_skew  act_bout_dur_entropy  inact_bout_dur_mean  \\\n",
       "45           5.337884              5.238865            21.813880   \n",
       "33           2.322436              5.096657            64.707143   \n",
       "40           2.332610              5.132767            46.642857   \n",
       "26           4.969473              5.324928            47.501312   \n",
       "11           3.084005              5.645285            29.305439   \n",
       "\n",
       "    inact_bout_dur_cv  inact_bout_dur_skew  inact_bout_dur_entropy  \n",
       "45           2.368584             5.583540                4.729848  \n",
       "33           2.424348             4.328888                4.389412  \n",
       "40           1.954293             4.103701                5.017741  \n",
       "26           1.585688             5.739879                5.259853  \n",
       "11           2.279229             4.874779                5.136790  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop unnecesarry column\n",
    "df_depression = df_features.drop(['number',\n",
    "                  'madrs2_grouped_ord',\n",
    "                  'Normal',\n",
    "                  'Mild',\n",
    "                  'Moderate'\n",
    "                  ], axis=1)\n",
    "df_depression.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>madrs2_grouped</th>\n",
       "      <th>activity_intraday_variability</th>\n",
       "      <th>activity_mean</th>\n",
       "      <th>activity_cv_minutely</th>\n",
       "      <th>activity_skewness_hourly</th>\n",
       "      <th>activity_entropy_hourly</th>\n",
       "      <th>activity_pct_0to6</th>\n",
       "      <th>activity_pct_6to12</th>\n",
       "      <th>activity_pct_12to18</th>\n",
       "      <th>...</th>\n",
       "      <th>act_bout_num</th>\n",
       "      <th>act_bout_dur_mean</th>\n",
       "      <th>act_bout_dur_std</th>\n",
       "      <th>act_bout_dur_cv</th>\n",
       "      <th>act_bout_dur_skew</th>\n",
       "      <th>act_bout_dur_entropy</th>\n",
       "      <th>inact_bout_dur_mean</th>\n",
       "      <th>inact_bout_dur_cv</th>\n",
       "      <th>inact_bout_dur_skew</th>\n",
       "      <th>inact_bout_dur_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>501.256833</td>\n",
       "      <td>264.990354</td>\n",
       "      <td>1.390468</td>\n",
       "      <td>2.319655</td>\n",
       "      <td>3.017930</td>\n",
       "      <td>0.285579</td>\n",
       "      <td>0.225209</td>\n",
       "      <td>0.349568</td>\n",
       "      <td>...</td>\n",
       "      <td>38.750000</td>\n",
       "      <td>15.570968</td>\n",
       "      <td>20.479364</td>\n",
       "      <td>1.315227</td>\n",
       "      <td>5.337884</td>\n",
       "      <td>5.238865</td>\n",
       "      <td>21.813880</td>\n",
       "      <td>2.368584</td>\n",
       "      <td>5.583540</td>\n",
       "      <td>4.729848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>581.136493</td>\n",
       "      <td>79.936852</td>\n",
       "      <td>2.697304</td>\n",
       "      <td>3.787372</td>\n",
       "      <td>2.147124</td>\n",
       "      <td>0.025093</td>\n",
       "      <td>0.400688</td>\n",
       "      <td>0.444788</td>\n",
       "      <td>...</td>\n",
       "      <td>17.923077</td>\n",
       "      <td>9.236052</td>\n",
       "      <td>9.194736</td>\n",
       "      <td>0.995527</td>\n",
       "      <td>2.322436</td>\n",
       "      <td>5.096657</td>\n",
       "      <td>64.707143</td>\n",
       "      <td>2.424348</td>\n",
       "      <td>4.328888</td>\n",
       "      <td>4.389412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>375.133788</td>\n",
       "      <td>69.371695</td>\n",
       "      <td>2.351840</td>\n",
       "      <td>3.688455</td>\n",
       "      <td>2.148713</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.343435</td>\n",
       "      <td>0.420807</td>\n",
       "      <td>...</td>\n",
       "      <td>16.153846</td>\n",
       "      <td>7.628571</td>\n",
       "      <td>5.674179</td>\n",
       "      <td>0.743806</td>\n",
       "      <td>2.332610</td>\n",
       "      <td>5.132767</td>\n",
       "      <td>46.642857</td>\n",
       "      <td>1.954293</td>\n",
       "      <td>4.103701</td>\n",
       "      <td>5.017741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>415.016644</td>\n",
       "      <td>75.205611</td>\n",
       "      <td>2.487839</td>\n",
       "      <td>3.412524</td>\n",
       "      <td>2.284191</td>\n",
       "      <td>0.311458</td>\n",
       "      <td>0.532379</td>\n",
       "      <td>0.269847</td>\n",
       "      <td>...</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>7.894737</td>\n",
       "      <td>7.177810</td>\n",
       "      <td>0.909189</td>\n",
       "      <td>4.969473</td>\n",
       "      <td>5.324928</td>\n",
       "      <td>47.501312</td>\n",
       "      <td>1.585688</td>\n",
       "      <td>5.739879</td>\n",
       "      <td>5.259853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>514.936507</td>\n",
       "      <td>165.284240</td>\n",
       "      <td>1.766114</td>\n",
       "      <td>2.869305</td>\n",
       "      <td>2.801761</td>\n",
       "      <td>0.043608</td>\n",
       "      <td>0.348435</td>\n",
       "      <td>0.451102</td>\n",
       "      <td>...</td>\n",
       "      <td>33.461538</td>\n",
       "      <td>10.245977</td>\n",
       "      <td>11.835146</td>\n",
       "      <td>1.155102</td>\n",
       "      <td>3.084005</td>\n",
       "      <td>5.645285</td>\n",
       "      <td>29.305439</td>\n",
       "      <td>2.279229</td>\n",
       "      <td>4.874779</td>\n",
       "      <td>5.136790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  madrs2_grouped  activity_intraday_variability  activity_mean  \\\n",
       "45          36               0                     501.256833     264.990354   \n",
       "33          13               2                     581.136493      79.936852   \n",
       "40          12               2                     375.133788      69.371695   \n",
       "26           5               1                     415.016644      75.205611   \n",
       "11          10               2                     514.936507     165.284240   \n",
       "\n",
       "    activity_cv_minutely  activity_skewness_hourly  activity_entropy_hourly  \\\n",
       "45              1.390468                  2.319655                 3.017930   \n",
       "33              2.697304                  3.787372                 2.147124   \n",
       "40              2.351840                  3.688455                 2.148713   \n",
       "26              2.487839                  3.412524                 2.284191   \n",
       "11              1.766114                  2.869305                 2.801761   \n",
       "\n",
       "    activity_pct_0to6  activity_pct_6to12  activity_pct_12to18  ...  \\\n",
       "45           0.285579            0.225209             0.349568  ...   \n",
       "33           0.025093            0.400688             0.444788  ...   \n",
       "40           0.055103            0.343435             0.420807  ...   \n",
       "26           0.311458            0.532379             0.269847  ...   \n",
       "11           0.043608            0.348435             0.451102  ...   \n",
       "\n",
       "    act_bout_num  act_bout_dur_mean  act_bout_dur_std  act_bout_dur_cv  \\\n",
       "45     38.750000          15.570968         20.479364         1.315227   \n",
       "33     17.923077           9.236052          9.194736         0.995527   \n",
       "40     16.153846           7.628571          5.674179         0.743806   \n",
       "26     19.000000           7.894737          7.177810         0.909189   \n",
       "11     33.461538          10.245977         11.835146         1.155102   \n",
       "\n",
       "    act_bout_dur_skew  act_bout_dur_entropy  inact_bout_dur_mean  \\\n",
       "45           5.337884              5.238865            21.813880   \n",
       "33           2.322436              5.096657            64.707143   \n",
       "40           2.332610              5.132767            46.642857   \n",
       "26           4.969473              5.324928            47.501312   \n",
       "11           3.084005              5.645285            29.305439   \n",
       "\n",
       "    inact_bout_dur_cv  inact_bout_dur_skew  inact_bout_dur_entropy  \n",
       "45           2.368584             5.583540                4.729848  \n",
       "33           2.424348             4.328888                4.389412  \n",
       "40           1.954293             4.103701                5.017741  \n",
       "26           1.585688             5.739879                5.259853  \n",
       "11           2.279229             4.874779                5.136790  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapping Y labels to numbers\n",
    "mapping = {'Normal': 0, 'Mild': 1, 'Moderate': 2}\n",
    "df_depression = df_depression.replace({'madrs2_grouped':mapping})\n",
    "label_names = ['Normal', 'Mild', 'Moderate']\n",
    "df_depression.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train:  (44, 19) \n",
      "Shape X_test:  (11, 19) \n",
      "Shape y_train:  (44,) \n",
      "Shape y_test: (11,)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "X = np.array(df_depression.iloc[:, 2:])\n",
    "y = np.array(df_depression.iloc[:, 1])\n",
    "\n",
    "\n",
    "# create samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,\n",
    "                                                    random_state=1, stratify=y)\n",
    "print('Shape X_train: ', X_train.shape, '\\nShape X_test: ', X_test.shape,\n",
    "      '\\nShape y_train: ', y_train.shape, '\\nShape y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler() \n",
    "# estimate the sample mean and standard deviation for each feature in X_train\n",
    "sc.fit(X_train)\n",
    "\n",
    "# use the two parameters to standardize both X_train and X_test\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_classes = 3, learning_rate=0.01):\n",
    "  \"\"\"Build a multi-class logistic regression model using Keras.\n",
    "\n",
    "  Args:\n",
    "    n_classes: Number of classes in the dataset\n",
    "    learning_rate: The desired learning rate for SGD.\n",
    "\n",
    "  Returns:\n",
    "    model: A tf.keras model (graph).\n",
    "  \"\"\"\n",
    "  tf.keras.backend.clear_session()\n",
    "  np.random.seed(0)\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten())\n",
    "  model.add(keras.layers.Dense(\n",
    "      units=n_classes,\n",
    "      use_bias=True,\n",
    "      activation='softmax'\n",
    "  ))\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                optimizer=optimizer, \n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3688 - accuracy: 0.4103"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 14:51:46.950899: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 260ms/step - loss: 1.3688 - accuracy: 0.4103 - val_loss: 1.6433 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3039 - accuracy: 0.4103 - val_loss: 1.5301 - val_accuracy: 0.2000\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2444 - accuracy: 0.4359 - val_loss: 1.4273 - val_accuracy: 0.2000\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.1902 - accuracy: 0.4615 - val_loss: 1.3339 - val_accuracy: 0.2000\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1412 - accuracy: 0.4615 - val_loss: 1.2494 - val_accuracy: 0.4000\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.0972 - accuracy: 0.5128 - val_loss: 1.1732 - val_accuracy: 0.4000\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0578 - accuracy: 0.5385 - val_loss: 1.1048 - val_accuracy: 0.4000\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.0227 - accuracy: 0.5385 - val_loss: 1.0438 - val_accuracy: 0.4000\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9912 - accuracy: 0.5641 - val_loss: 0.9898 - val_accuracy: 0.4000\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9630 - accuracy: 0.5641 - val_loss: 0.9423 - val_accuracy: 0.4000\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9375 - accuracy: 0.5641 - val_loss: 0.9006 - val_accuracy: 0.4000\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9145 - accuracy: 0.5641 - val_loss: 0.8642 - val_accuracy: 0.4000\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.8934 - accuracy: 0.5897 - val_loss: 0.8325 - val_accuracy: 0.4000\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.8741 - accuracy: 0.6154 - val_loss: 0.8050 - val_accuracy: 0.6000\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.8561 - accuracy: 0.6410 - val_loss: 0.7810 - val_accuracy: 0.6000\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8395 - accuracy: 0.6667 - val_loss: 0.7602 - val_accuracy: 0.8000\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8238 - accuracy: 0.6667 - val_loss: 0.7421 - val_accuracy: 0.8000\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8090 - accuracy: 0.6667 - val_loss: 0.7263 - val_accuracy: 0.8000\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7949 - accuracy: 0.6667 - val_loss: 0.7125 - val_accuracy: 0.8000\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7814 - accuracy: 0.6667 - val_loss: 0.7004 - val_accuracy: 0.8000\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7685 - accuracy: 0.6667 - val_loss: 0.6898 - val_accuracy: 0.8000\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7560 - accuracy: 0.6667 - val_loss: 0.6805 - val_accuracy: 0.8000\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7440 - accuracy: 0.6667 - val_loss: 0.6723 - val_accuracy: 0.8000\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7324 - accuracy: 0.6667 - val_loss: 0.6652 - val_accuracy: 0.8000\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7211 - accuracy: 0.6667 - val_loss: 0.6591 - val_accuracy: 0.8000\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7102 - accuracy: 0.6667 - val_loss: 0.6538 - val_accuracy: 0.8000\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6997 - accuracy: 0.6667 - val_loss: 0.6494 - val_accuracy: 0.8000\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6895 - accuracy: 0.6923 - val_loss: 0.6459 - val_accuracy: 0.8000\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6796 - accuracy: 0.7179 - val_loss: 0.6432 - val_accuracy: 0.8000\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6701 - accuracy: 0.7179 - val_loss: 0.6412 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.368817</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>1.643303</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.303927</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>1.530076</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.244397</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>1.427275</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.190194</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.333935</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.141212</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.249409</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.097211</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>1.173156</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.057843</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.104762</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.022673</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.043810</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.991216</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.989828</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.962989</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.942282</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.937539</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.900604</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.914462</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.864209</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.893403</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.832516</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.874054</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.804964</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.856148</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.781027</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.839456</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.760220</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.823785</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.742111</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.808971</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.726317</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.794883</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.712509</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.781413</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.700413</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.768480</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.756022</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.680486</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.743993</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.672328</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.732361</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.665217</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.721105</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.659069</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.710210</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.653825</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.699667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.649443</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.689467</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.645893</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.679607</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.643154</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.670079</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.641211</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   1.368817  0.410256  1.643303           0.0\n",
       "1   1.303927  0.410256  1.530076           0.2\n",
       "2   1.244397  0.435897  1.427275           0.2\n",
       "3   1.190194  0.461538  1.333935           0.2\n",
       "4   1.141212  0.461538  1.249409           0.4\n",
       "5   1.097211  0.512821  1.173156           0.4\n",
       "6   1.057843  0.538462  1.104762           0.4\n",
       "7   1.022673  0.538462  1.043810           0.4\n",
       "8   0.991216  0.564103  0.989828           0.4\n",
       "9   0.962989  0.564103  0.942282           0.4\n",
       "10  0.937539  0.564103  0.900604           0.4\n",
       "11  0.914462  0.564103  0.864209           0.4\n",
       "12  0.893403  0.589744  0.832516           0.4\n",
       "13  0.874054  0.615385  0.804964           0.6\n",
       "14  0.856148  0.641026  0.781027           0.6\n",
       "15  0.839456  0.666667  0.760220           0.8\n",
       "16  0.823785  0.666667  0.742111           0.8\n",
       "17  0.808971  0.666667  0.726317           0.8\n",
       "18  0.794883  0.666667  0.712509           0.8\n",
       "19  0.781413  0.666667  0.700413           0.8\n",
       "20  0.768480  0.666667  0.689799           0.8\n",
       "21  0.756022  0.666667  0.680486           0.8\n",
       "22  0.743993  0.666667  0.672328           0.8\n",
       "23  0.732361  0.666667  0.665217           0.8\n",
       "24  0.721105  0.666667  0.659069           0.8\n",
       "25  0.710210  0.666667  0.653825           0.8\n",
       "26  0.699667  0.666667  0.649443           0.8\n",
       "27  0.689467  0.692308  0.645893           0.8\n",
       "28  0.679607  0.717949  0.643154           0.8\n",
       "29  0.670079  0.717949  0.641211           0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(3,0.01)\n",
    "\n",
    "hist = model.fit(\n",
    "  x = X_train_std,\n",
    "  y = y_train,\n",
    "  epochs=30,\n",
    "  batch_size=64,\n",
    "  validation_split=0.1,\n",
    "  verbose=1)\n",
    "\n",
    "history = pd.DataFrame(hist.history)\n",
    "display(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grab history\n",
    "history = hist.history\n",
    "\n",
    "# plot loss for train and validation\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "plt.plot(history['loss'], lw=2, color='darkgoldenrod')\n",
    "plt.plot(history['val_loss'], lw=2, color='indianred')\n",
    "plt.legend(['Train', 'Validation'], fontsize=10)\n",
    "ax.set_xlabel('Epochs', size=10)\n",
    "ax.set_title('Loss');\n",
    "\n",
    "# plot accuracy for train and validation\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "plt.plot(history['accuracy'], lw=2, color='darkgoldenrod')\n",
    "plt.plot(history['val_accuracy'], lw=2, color='indianred')\n",
    "plt.legend(['Train', 'Validation'], fontsize=10)\n",
    "ax.set_xlabel('Epochs', size=10)\n",
    "ax.set_title('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "[2 1 2 0 0 0 2 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Evaluation\n",
    "test_predictions = np.argmax(model.predict(X_test_std), axis=-1)\n",
    "print(test_predictions)\n",
    "\n",
    "# Create a confusion matrix as a 2D array.\n",
    "confusion_matrix = tf.math.confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "# Use a heatmap plot to display it.\n",
    "ax = sns.heatmap(confusion_matrix, annot=True, fmt='.3g', cmap='Blues',\n",
    "                 xticklabels=label_names, yticklabels=label_names, cbar=False)\n",
    "\n",
    "# Add axis labels.\n",
    "ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal precision: tf.Tensor(71.42857142857143, shape=(), dtype=float64)\n",
      "Mild precision: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Moderate precision: tf.Tensor(0.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "#calculate precision for each class\n",
    "for i,val in enumerate(label_names):\n",
    "    precision = 100* confusion_matrix[i,i] / np.sum(confusion_matrix[:,i])\n",
    "    print(val, \"precision:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial formulation of the boosting algorithm (4 key steps):<br>\n",
    " 1. Draw a random sample of training examples, 𝑑𝑑1, without\n",
    "replacement from the training dataset, D, to train a weak learner, 𝐶𝐶1.\n",
    " 2. Draw a second random sample of training examples, 𝑑𝑑2, without replacement from the\n",
    "training dataset and add 50 percent of the examples that were previously\n",
    "misclassified to train a weak learner, 𝐶𝐶2.\n",
    " 3. Find the training examples, 𝑑𝑑3, in the training dataset, D, which 𝐶𝐶1 and 𝐶𝐶2\n",
    "disagree upon, to train a third weak learner, 𝐶𝐶3.\n",
    " 4. Combine the weak learners 𝐶𝐶1, 𝐶𝐶2, and 𝐶𝐶3 via majority voting.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is AdaBoost different (see Sebastian's book, pp. 250-257):\n",
    "* uses all the training examples to train the weak learners\n",
    "* the training examples are reweighed in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a stump decision tree \n",
    "tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                              max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the stump decision tree as the base clasifier, and create an ensemble of 500 stump decision trees \n",
    "#fit on different bootsrap sambles of the training dataset (w/o replacement).\n",
    "ada = AdaBoostClassifier(base_estimator=tree,\n",
    "                         n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost train/test accuracies 0.841/0.455\n"
     ]
    }
   ],
   "source": [
    "#Perform and evaluate the algorithm\n",
    "ada = ada.fit(X_train_std, y_train)\n",
    "y_train_pred = ada.predict(X_train_std)\n",
    "y_test_pred = ada.predict(X_test_std)\n",
    "\n",
    "ada_train = accuracy_score(y_train, y_train_pred) \n",
    "ada_test = accuracy_score(y_test, y_test_pred) \n",
    "print('AdaBoost train/test accuracies %.3f/%.3f'\n",
    "      % (ada_train, ada_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression has lower training accuracy than Adaboost (0.8<0.841) but higher testing accuracy(0.54>0.455). This shows Adaboost is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost and gradient boosting share the main overall concept: boosting weak learners (such as decision tree stumps) to strong learners. The two approaches, adaptive and gradient boosting, differ mainly with regard to how the weights are updated and how the (weak) classifiers are combined. XGBoost is an optimized gradient boosting\n",
    "\n",
    "In gradient boosting, the trees are built in a sequential manner such that each subsequent tree aims to reduce the errors of the previous tree. The misclassified labels are given higher weights. Each tree learns from its predecessors and tries to reduce the residual errors. So, the tree next in sequence will learn from the previous tree residuals.\n",
    "\n",
    "In XGBoost, we try to fit a model on the gradient of the loss function generated from the previous step. So, in XGBoost we modified our gradient boosting algorithm so that it works with any differentiable loss function.\n",
    "\n",
    "https://gist.github.com/pb111/cc341409081dffa5e9eaf60d79562a03\n",
    "\n",
    "https://www.kaggle.com/code/stuarthallows/using-xgboost-with-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              objective='multi:softprob', predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, scale_pos_weight=None,\n",
      "              subsample=None, tree_method=None, use_label_encoder='False',\n",
      "              validate_parameters=None, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "# declare parameters\n",
    "params = {\n",
    "            'objective':'multi:softprob',\n",
    "            'use_label_encoder':'False'\n",
    "            #'max_depth': 4,\n",
    "            #'alpha': 10,\n",
    "            #'learning_rate': 1.0,\n",
    "            #'n_estimators':100\n",
    "        }\n",
    "# instantiate the classifier \n",
    "xgb_clf = xgb.XGBClassifier(**params)\n",
    "print(xgb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:52:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost train/test accuracies 1.000/0.455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#Perform and evaluate the algorithm\n",
    "xg = xgb_clf.fit(X_train_std, y_train)\n",
    "y_train_pred = xg.predict(X_train_std)\n",
    "y_test_pred = xg.predict(X_test_std)\n",
    "\n",
    "xg_train = accuracy_score(y_train, y_train_pred) \n",
    "xg_test = accuracy_score(y_test, y_test_pred) \n",
    "print('XGBoost train/test accuracies %.3f/%.3f'\n",
    "      % (xg_train, xg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/nhanle/miniforge3/envs/w207_final/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:20:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0.19123099563358137, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.32496926574203644,\n",
      "              max_delta_step=0, max_depth=2, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=102, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=0.9439761626945282, tree_method='exact',\n",
      "              use_label_encoder='e', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "params = {\n",
    "    \"gamma\": uniform(0, 0.5),\n",
    "    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "    \"max_depth\": randint(2, 6), # default 3\n",
    "    \"n_estimators\": randint(100, 150), # default 100\n",
    "    'use_label_encoder':'False',\n",
    "    \"subsample\": uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=10, cv=5)\n",
    "\n",
    "search.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0.19123099563358137, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.32496926574203644,\n",
      "              max_delta_step=0, max_depth=2, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=102, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=0.9439761626945282, tree_method='exact',\n",
      "              use_label_encoder='e', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "best_rf = search.best_estimator_\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare parameters\n",
    "params = {\n",
    "            'objective':'multi:softprob',\n",
    "            'use_label_encoder':'False'\n",
    "            #'max_depth': 4,\n",
    "            #'alpha': 10,\n",
    "            #'learning_rate': 1.0,\n",
    "            #'n_estimators':100\n",
    "        }\n",
    "# instantiate the classifier \n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest train/test accuracies 1.000/0.545\n"
     ]
    }
   ],
   "source": [
    "#Perform and evaluate the algorithm\n",
    "rf = rf_clf.fit(X_train_std, y_train)\n",
    "y_train_pred = rf.predict(X_train_std)\n",
    "y_test_pred = rf.predict(X_test_std)\n",
    "\n",
    "rf_train = accuracy_score(y_train, y_train_pred) \n",
    "rf_test = accuracy_score(y_test, y_test_pred) \n",
    "print('Random Forest train/test accuracies %.3f/%.3f'\n",
    "      % (rf_train, rf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
